{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using backend: pytorch\n"
     ]
    }
   ],
   "source": [
    "import dgl\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node feature dimensionality: 3\n",
      "Number of graph categories: 2\n"
     ]
    }
   ],
   "source": [
    "# Generate a synthetic dataset with 10000 graphs, ranging from 10 to 500 nodes\n",
    "import dgl.data\n",
    "\n",
    "dataset = dgl.data.GINDataset('PROTEINS', self_loop=True)\n",
    "print('Node feature dimensionality:', dataset.dim_nfeats)\n",
    "print('Number of graph categories:', dataset.gclasses)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Graph(num_nodes=1006, num_edges=4572,\n",
      "      ndata_schemes={'label': Scheme(shape=(), dtype=torch.int64), 'attr': Scheme(shape=(3,), dtype=torch.float32)}\n",
      "      edata_schemes={}), tensor([0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0])]\n"
     ]
    }
   ],
   "source": [
    "#Load data with mini-batches\n",
    "\n",
    "from dgl.dataloading import GraphDataLoader\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "\n",
    "num_samples = len(dataset)\n",
    "num_train_samples = int(0.8 * num_samples)\n",
    "num_valid_samples = int(0.1 * num_samples)\n",
    "num_test_samples = int(0.1 * num_samples)\n",
    "\n",
    "# Choose a data sampler\n",
    "# There are many other options, see details below:\n",
    "# https://pytorch.org/docs/stable/data.html#data-loading-order-and-sampler\n",
    "train_sampler = SubsetRandomSampler(torch.arange(num_train_samples))\n",
    "valid_sampler = SubsetRandomSampler(torch.arange(num_train_samples, num_train_samples+num_valid_samples))\n",
    "test_sampler = SubsetRandomSampler(torch.arange(num_train_samples+num_valid_samples, num_train_samples+num_valid_samples+num_test_samples))\n",
    "\n",
    "train_loader = GraphDataLoader(\n",
    "    dataset, sampler=train_sampler, batch_size=16, drop_last=False)\n",
    "valid_loader = GraphDataLoader(\n",
    "    dataset, sampler=valid_sampler, batch_size=16, drop_last=False)\n",
    "test_loader = GraphDataLoader(\n",
    "    dataset, sampler=test_sampler, batch_size=16, drop_last=False)\n",
    "\n",
    "# Check datapoint\n",
    "item = iter(train_loader)\n",
    "batch = next(item)\n",
    "print(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nodes for each graph: tensor([246, 126,  43,  71,  43,   6,  18,  27, 146,  23,  50,   6, 144,  13,\n",
      "         18,  26])\n",
      "Number of edges for each graph: tensor([1116,  656,  195,  317,  199,   30,   80,  149,  628,  107,  226,   28,\n",
      "         582,   63,   80,  116])\n",
      "The original graphs in the minibatch: [Graph(num_nodes=246, num_edges=1116,\n",
      "      ndata_schemes={'label': Scheme(shape=(), dtype=torch.int64), 'attr': Scheme(shape=(3,), dtype=torch.float32)}\n",
      "      edata_schemes={}), Graph(num_nodes=126, num_edges=656,\n",
      "      ndata_schemes={'label': Scheme(shape=(), dtype=torch.int64), 'attr': Scheme(shape=(3,), dtype=torch.float32)}\n",
      "      edata_schemes={}), Graph(num_nodes=43, num_edges=195,\n",
      "      ndata_schemes={'label': Scheme(shape=(), dtype=torch.int64), 'attr': Scheme(shape=(3,), dtype=torch.float32)}\n",
      "      edata_schemes={}), Graph(num_nodes=71, num_edges=317,\n",
      "      ndata_schemes={'label': Scheme(shape=(), dtype=torch.int64), 'attr': Scheme(shape=(3,), dtype=torch.float32)}\n",
      "      edata_schemes={}), Graph(num_nodes=43, num_edges=199,\n",
      "      ndata_schemes={'label': Scheme(shape=(), dtype=torch.int64), 'attr': Scheme(shape=(3,), dtype=torch.float32)}\n",
      "      edata_schemes={}), Graph(num_nodes=6, num_edges=30,\n",
      "      ndata_schemes={'label': Scheme(shape=(), dtype=torch.int64), 'attr': Scheme(shape=(3,), dtype=torch.float32)}\n",
      "      edata_schemes={}), Graph(num_nodes=18, num_edges=80,\n",
      "      ndata_schemes={'label': Scheme(shape=(), dtype=torch.int64), 'attr': Scheme(shape=(3,), dtype=torch.float32)}\n",
      "      edata_schemes={}), Graph(num_nodes=27, num_edges=149,\n",
      "      ndata_schemes={'label': Scheme(shape=(), dtype=torch.int64), 'attr': Scheme(shape=(3,), dtype=torch.float32)}\n",
      "      edata_schemes={}), Graph(num_nodes=146, num_edges=628,\n",
      "      ndata_schemes={'label': Scheme(shape=(), dtype=torch.int64), 'attr': Scheme(shape=(3,), dtype=torch.float32)}\n",
      "      edata_schemes={}), Graph(num_nodes=23, num_edges=107,\n",
      "      ndata_schemes={'label': Scheme(shape=(), dtype=torch.int64), 'attr': Scheme(shape=(3,), dtype=torch.float32)}\n",
      "      edata_schemes={}), Graph(num_nodes=50, num_edges=226,\n",
      "      ndata_schemes={'label': Scheme(shape=(), dtype=torch.int64), 'attr': Scheme(shape=(3,), dtype=torch.float32)}\n",
      "      edata_schemes={}), Graph(num_nodes=6, num_edges=28,\n",
      "      ndata_schemes={'label': Scheme(shape=(), dtype=torch.int64), 'attr': Scheme(shape=(3,), dtype=torch.float32)}\n",
      "      edata_schemes={}), Graph(num_nodes=144, num_edges=582,\n",
      "      ndata_schemes={'label': Scheme(shape=(), dtype=torch.int64), 'attr': Scheme(shape=(3,), dtype=torch.float32)}\n",
      "      edata_schemes={}), Graph(num_nodes=13, num_edges=63,\n",
      "      ndata_schemes={'label': Scheme(shape=(), dtype=torch.int64), 'attr': Scheme(shape=(3,), dtype=torch.float32)}\n",
      "      edata_schemes={}), Graph(num_nodes=18, num_edges=80,\n",
      "      ndata_schemes={'label': Scheme(shape=(), dtype=torch.int64), 'attr': Scheme(shape=(3,), dtype=torch.float32)}\n",
      "      edata_schemes={}), Graph(num_nodes=26, num_edges=116,\n",
      "      ndata_schemes={'label': Scheme(shape=(), dtype=torch.int64), 'attr': Scheme(shape=(3,), dtype=torch.float32)}\n",
      "      edata_schemes={})]\n"
     ]
    }
   ],
   "source": [
    "graph_batch, labels = batch\n",
    "print('Number of nodes for each graph:', graph_batch.batch_num_nodes())\n",
    "print('Number of edges for each graph:', graph_batch.batch_num_edges())\n",
    "\n",
    "# Reconstruct original graphs in a mini-batch\n",
    "ori_graphs = dgl.unbatch(graph_batch)\n",
    "print('The original graphs in the minibatch:', ori_graphs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build models\n",
    "\n",
    "from dgl.nn import GraphConv, GATConv, GINConv\n",
    "\n",
    "# Graph Convolutional Networks\n",
    "# TODO: more options for graph pooling\n",
    "class GCN(nn.Module):\n",
    "    def __init__(self, in_feats, h_feats, num_classes):\n",
    "        super(GCN, self).__init__()\n",
    "        self.conv1 = GraphConv(in_feats, h_feats)\n",
    "        self.conv2 = GraphConv(h_feats, num_classes)\n",
    "        \n",
    "    def forward(self, g, in_feat):\n",
    "        h = self.conv1(g, in_feat)\n",
    "        h = F.relu(h)\n",
    "        h = self.conv2(g, h)\n",
    "        g.ndata['h'] = h\n",
    "        return dgl.mean_nodes(g, 'h')\n",
    "    \n",
    "# Graph Attention Networks\n",
    "class GAT(nn.Module):\n",
    "    def __init__(self, in_feats, h_feats, num_classes):\n",
    "        super(GAT, self).__init__()\n",
    "        self.conv1 = GATConv(in_feats, h_feats, num_heads=3)\n",
    "        self.conv2 = GATConv(h_feats, num_classes, num_heads=3)\n",
    "        \n",
    "    def forward(self, g, in_feat):\n",
    "        h = self.conv1(g, in_feat)\n",
    "        h = F.relu(h)\n",
    "        h = torch.mean(h, dim=1) # Average value over three attention heads\n",
    "        h = self.conv2(g, h)\n",
    "        h = torch.mean(h, dim=1) # Average value over three attention heads\n",
    "        g.ndata['h'] = h\n",
    "        return dgl.mean_nodes(g, 'h')\n",
    "    \n",
    "# Graph Isomorphism Networks\n",
    "class GIN(nn.Module):\n",
    "    def __init__(self, in_feats, h_feats, num_classes):\n",
    "        super(GIN, self).__init__()\n",
    "        lin1 = torch.nn.Linear(in_feats, h_feats)\n",
    "        lin2 = torch.nn.Linear(h_feats, num_classes)\n",
    "        self.conv1 = GINConv(lin1, 'sum')\n",
    "        self.conv2 = GINConv(lin2, 'sum')\n",
    "        \n",
    "    def forward(self, g, in_feat):\n",
    "        h = self.conv1(g, in_feat)\n",
    "        h = F.relu(h)\n",
    "        h = self.conv2(g, h)\n",
    "        g.ndata['h'] = h\n",
    "        return dgl.mean_nodes(g, 'h')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid accuracy: 0.2882882882882883\n",
      "Test accuracy: 0.2702702702702703\n"
     ]
    }
   ],
   "source": [
    "# Train models\n",
    "\n",
    "model_gcn = GCN(dataset.dim_nfeats, 16, dataset.gclasses)\n",
    "\n",
    "optimizer = torch.optim.Adam(model_gcn.parameters(), lr=0.01)\n",
    "\n",
    "for epoch in range(20):\n",
    "    for graph_batch, labels in train_loader:\n",
    "        pred = model_gcn(graph_batch, graph_batch.ndata['attr'].float())\n",
    "        loss = F.cross_entropy(pred, labels)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "num_correct = 0\n",
    "num_valids = 0\n",
    "for graph_batch, labels in valid_loader:\n",
    "    pred = model_gcn(graph_batch, graph_batch.ndata['attr'].float())\n",
    "    num_correct += (pred.argmax(1) == labels).sum().item()\n",
    "    num_valids += len(labels)        \n",
    "\n",
    "print('Valid accuracy:', num_correct / num_valids)\n",
    "    \n",
    "num_correct = 0\n",
    "num_tests = 0\n",
    "for graph_batch, labels in test_loader:\n",
    "    pred = model_gcn(graph_batch, graph_batch.ndata['attr'].float())\n",
    "    num_correct += (pred.argmax(1) == labels).sum().item()\n",
    "    num_tests += len(labels)\n",
    "\n",
    "print('Test accuracy:', num_correct / num_tests)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid accuracy: 0.2972972972972973\n",
      "Test accuracy: 0.27927927927927926\n"
     ]
    }
   ],
   "source": [
    "# Train GAT model\n",
    "\n",
    "model_gat = GAT(dataset.dim_nfeats, 16, dataset.gclasses)\n",
    "\n",
    "optimizer = torch.optim.Adam(model_gat.parameters(), lr=0.01)\n",
    "\n",
    "for epoch in range(20):\n",
    "    for graph_batch, labels in train_loader:\n",
    "        pred = model_gat(graph_batch, graph_batch.ndata['attr'].float())\n",
    "        loss = F.cross_entropy(pred, labels)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "num_correct = 0\n",
    "num_valids = 0\n",
    "for graph_batch, labels in valid_loader:\n",
    "    pred = model_gat(graph_batch, graph_batch.ndata['attr'].float())\n",
    "    num_correct += (pred.argmax(1) == labels).sum().item()\n",
    "    num_valids += len(labels)        \n",
    "\n",
    "print('Valid accuracy:', num_correct / num_valids)\n",
    "    \n",
    "num_correct = 0\n",
    "num_tests = 0\n",
    "for graph_batch, labels in test_loader:\n",
    "    pred = model_gat(graph_batch, graph_batch.ndata['attr'].float())\n",
    "    num_correct += (pred.argmax(1) == labels).sum().item()\n",
    "    num_tests += len(labels)\n",
    "\n",
    "print('Test accuracy:', num_correct / num_tests)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid accuracy: 0.2702702702702703\n",
      "Test accuracy: 0.24324324324324326\n"
     ]
    }
   ],
   "source": [
    "# Train GIN model\n",
    "\n",
    "model_gin = GIN(dataset.dim_nfeats, 16, dataset.gclasses)\n",
    "\n",
    "optimizer = torch.optim.Adam(model_gin.parameters(), lr=0.01)\n",
    "\n",
    "for epoch in range(20):\n",
    "    for graph_batch, labels in train_loader:\n",
    "        pred = model_gin(graph_batch, graph_batch.ndata['attr'].float())\n",
    "        loss = F.cross_entropy(pred, labels)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "num_correct = 0\n",
    "num_valids = 0\n",
    "for graph_batch, labels in valid_loader:\n",
    "    pred = model_gin(graph_batch, graph_batch.ndata['attr'].float())\n",
    "    num_correct += (pred.argmax(1) == labels).sum().item()\n",
    "    num_valids += len(labels)        \n",
    "\n",
    "print('Valid accuracy:', num_correct / num_valids)\n",
    "    \n",
    "num_correct = 0\n",
    "num_tests = 0\n",
    "for graph_batch, labels in test_loader:\n",
    "    pred = model_gin(graph_batch, graph_batch.ndata['attr'].float())\n",
    "    num_correct += (pred.argmax(1) == labels).sum().item()\n",
    "    num_tests += len(labels)\n",
    "\n",
    "print('Test accuracy:', num_correct / num_tests)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
