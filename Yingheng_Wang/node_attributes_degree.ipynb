{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import csv\n",
    "import re\n",
    "%matplotlib inline\n",
    "\n",
    "graph_idx = 0\n",
    "pos_idx = []\n",
    "neg_idx = []\n",
    "f = csv.reader(open('./BNU1_fMRI/BNU1_phenotypic_data.csv','r')) \n",
    "for j, row in enumerate(f):\n",
    "    if j != 0 and j % 2 ==1:\n",
    "        label = row[3]\n",
    "        filter(str.isdigit, label)\n",
    "        if int(label)-1 == 1:\n",
    "            pos_idx.append(graph_idx)\n",
    "        else:\n",
    "            neg_idx.append(graph_idx)\n",
    "        graph_idx += 1\n",
    "\n",
    "edge_list_2_downsampling = []\n",
    "edge_list_2_property = [] \n",
    "pos_adj_list = []\n",
    "neg_adj_list = []\n",
    "adj_list = []\n",
    "labels = []\n",
    "length = 25920 - 25864 + 1\n",
    "for i in range(length):\n",
    "    G00 = nx.read_gpickle(\"./BNU1_fMRI/sub-00%s_ses-1_bold_desikan_res-2x2x2_measure-correlation.gpickle\"%(int(i+25864)))\n",
    "    edge_list_2 = []\n",
    "    edge = G00.adj\n",
    "    for src in edge:\n",
    "        for dst in edge[src]:\n",
    "            edge_list_2.append([src, dst, edge[src][dst]['weight']])\n",
    "    for edge in edge_list_2:\n",
    "        if edge[0] == edge[1]:\n",
    "            continue\n",
    "        elif edge[2] <= 0.8:\n",
    "            edge_list_2_downsampling.append([edge[0]-1, edge[1]-1, 0])\n",
    "        else:\n",
    "            edge_list_2_downsampling.append([edge[0]-1, edge[1]-1, edge[2]])\n",
    "    G2 = nx.Graph()\n",
    "    G2.add_weighted_edges_from(edge_list_2_downsampling)\n",
    "    A2 = nx.to_numpy_array(G2)\n",
    "    adj_list.append(A2)\n",
    "    if i in pos_idx:\n",
    "        labels.append('male')\n",
    "        pos_adj_list.append(A2)\n",
    "#         np.stack((pos_adj_list, A2),axis=0)\n",
    "    else:\n",
    "        labels.append('female')\n",
    "        neg_adj_list.append(A2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Loading from dataset\n",
      "./imdb_binary.graph\n",
      "#\n",
      "#\n",
      "#\n",
      "#\n",
      "#\n",
      "#\n",
      "#\n",
      "#\n",
      "#\n",
      "#\n",
      "#\n",
      "#\n",
      "#\n",
      "#\n",
      "#\n",
      "#\n",
      "#\n",
      "#\n",
      "#\n",
      "#\n",
      "Number of bins are 50\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'statsmodels'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-39-45b9cc0b7c0b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    327\u001b[0m \u001b[1;31m# save_graphs_(graphs_, dataset=dataset, norm_flag=norm_flag)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    328\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 329\u001b[1;33m \u001b[0mx_original\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmerge_features\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgraphs_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbl_feat\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_bin\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhis_norm_flag\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mhis_norm_flag\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcdf_flag\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcdf_flag\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0muniform_flag\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0muniform_flag\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    330\u001b[0m \u001b[1;31m# if norm_flag=='yes':\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    331\u001b[0m \u001b[1;31m#     x = normalize(x_original, axis = 1)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-39-45b9cc0b7c0b>\u001b[0m in \u001b[0;36mmerge_features\u001b[1;34m(graph, graphs_, allowed, n_bin, his_norm_flag, edge_flag, cdf_flag, uniform_flag)\u001b[0m\n\u001b[0;32m    284\u001b[0m             \u001b[0mtmp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhisgram_single_feature\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgraphs_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_bin\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhis_norm_flag\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mhis_norm_flag\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0medge_flag\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0medge_flag\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcdf_flag\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcdf_flag\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0muniform_flag\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0muniform_flag\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    285\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 286\u001b[1;33m             \u001b[0mtmp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhisgram_single_feature\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgraphs_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_bin\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhis_norm_flag\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mhis_norm_flag\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0medge_flag\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0medge_flag\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcdf_flag\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcdf_flag\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0muniform_flag\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0muniform_flag\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlowerbound\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    287\u001b[0m         \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtmp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    288\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mremove_zero_col\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-39-45b9cc0b7c0b>\u001b[0m in \u001b[0;36mhisgram_single_feature\u001b[1;34m(graphs_, n_bin, key, his_norm_flag, edge_flag, lowerbound, upperbound, cdf_flag, uniform_flag)\u001b[0m\n\u001b[0;32m    215\u001b[0m         feature_vec[i] = hisgram(lis, n_bin, his_norm_flag=his_norm_flag,\n\u001b[0;32m    216\u001b[0m                                  \u001b[0mlowerbound\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlowerbound\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mupperbound\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mupperbound\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 217\u001b[1;33m                                  cdf_flag=cdf_flag, uniform_flag=uniform_flag)\n\u001b[0m\u001b[0;32m    218\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mfeature_vec\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    219\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-39-45b9cc0b7c0b>\u001b[0m in \u001b[0;36mhisgram\u001b[1;34m(lis, n_bin, his_norm_flag, lowerbound, upperbound, cdf_flag, uniform_flag)\u001b[0m\n\u001b[0;32m    242\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    243\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mcdf_flag\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 244\u001b[1;33m         \u001b[1;32mfrom\u001b[0m \u001b[0mstatsmodels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdistributions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mempirical_distribution\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mECDF\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    245\u001b[0m         \u001b[0mecdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mECDF\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    246\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0muniform_flag\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'statsmodels'"
     ]
    }
   ],
   "source": [
    "import networkx as nx\n",
    "from joblib import Parallel, delayed\n",
    "import pickle\n",
    "import os, sys\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "def make_direct(direct):\n",
    "    # has side effect\n",
    "    import os\n",
    "    if not os.path.exists(direct):\n",
    "            os.makedirs(direct)\n",
    "\n",
    "def load_graph(graph, debug='off', single_graph_flag=True):\n",
    "    # exptect label to be numpy.ndarry of shape (n,). However protein_data is different so have to handle it differently\n",
    "    assert type(graph) == str\n",
    "    print('Start Loading from dataset')\n",
    "    file = os.path.join(graph + \".graph\")\n",
    "    print(file)\n",
    "    # if not os.path.isfile(file): file = '/home/cai.507/Documents/DeepLearning/deep-persistence/dataset/datasets/' + graph + '.graph'\n",
    "    f = open(file, 'rb')\n",
    "    data = pickle.load(f, encoding='bytes')\n",
    "#     print(data)\n",
    "    graphs, labels = data[b'graph'], data[b'labels']\n",
    "    return graphs, labels\n",
    "\n",
    "    if debug == 'on':\n",
    "        print(graph),\n",
    "        print(type(labels), )\n",
    "        print(np.shape(labels))\n",
    "    print('Finish Loading graphs')\n",
    "    outputFile = directory + '\\graph+label'\n",
    "    fw = open(outputFile, 'wb')\n",
    "    dataset = (graphs, labels)\n",
    "    pickle.dump(dataset, fw)\n",
    "    fw.close()\n",
    "    print('Finish Saving data for future use')\n",
    "    return graphs, labels\n",
    "\n",
    "def convert2nx(graph, i, print_flag='False'):\n",
    "    # graph: python dict\n",
    "    keys = graph.keys()\n",
    "    try:\n",
    "        assert keys == range(len(graph.keys()))\n",
    "    except AssertionError:\n",
    "        # print('%s graph has non consecutive keys'%i)\n",
    "        # print('Missing nodes are the follwing:')\n",
    "        for i in range(max(graph.keys())):\n",
    "            if i not in graph.keys(): \n",
    "                print(i)\n",
    "\n",
    "    # add nodes\n",
    "    gi = nx.Graph()\n",
    "    for i in keys: gi.add_node(i) # change from 1 to i.\n",
    "    assert len(gi) == len(keys)\n",
    "\n",
    "    # add edges\n",
    "    for i in keys:\n",
    "        for j in graph[i][b'neighbors']:\n",
    "            if j > i:\n",
    "                gi.add_edge(i, j)\n",
    "    for i in keys:\n",
    "        # print graph[i]['label']\n",
    "        if graph[i][b'label']=='':\n",
    "            print(gi)\n",
    "            gi.nodes[i]['label'] = 1\n",
    "            # continue\n",
    "        try:\n",
    "            gi.nodes[i]['label'] = graph[i][b'label'][0]\n",
    "        except TypeError: # modifications for reddit_binary\n",
    "            gi.nodes[i]['label'] = graph[i][b'label']\n",
    "        except IndexError:\n",
    "            gi.nodes[i]['label'] = 0 # modification for imdb_binary\n",
    "    assert len(gi.nodes) == len(graph.keys())\n",
    "    gi.remove_edges_from(nx.selfloop_edges(gi))\n",
    "    if print_flag=='True': print('graph: %s, n_nodes: %s, n_edges: %s' %(i, len(gi), len(gi.edges)) )\n",
    "    return gi\n",
    "\n",
    "def attribute_mean(g, i, key='deg', cutoff=1, iteration=0):\n",
    "    # g = graphs_[i][0]\n",
    "    # g = graphs_[0][0]\n",
    "    # attribute_mean(g, 0, iteration=1)\n",
    "    for itr in [iteration]:\n",
    "        assert key in g.nodes[i].keys()\n",
    "        # nodes_b = nx.single_source_shortest_path_length(g,i,cutoff=cutoff).keys()\n",
    "        # nodes_a = nx.single_source_shortest_path_length(g,i,cutoff=cutoff-1).keys()\n",
    "        # nodes = [k for k in nodes_b if k not in nodes_a]\n",
    "        nodes = g[i].keys()\n",
    "\n",
    "        if iteration == 0:\n",
    "            nbrs_deg = [g.nodes[j][key] for j in nodes]\n",
    "        else:\n",
    "            key_ = str(cutoff) + '_' + str(itr-1) + '_' + key +  '_' + 'mean'\n",
    "            nbrs_deg = [g.nodes[j][key_] for j in nodes]\n",
    "            g.nodes[i][ str(cutoff) + '_' + str(itr) + '_' + key] = np.mean(nbrs_deg)\n",
    "            return\n",
    "\n",
    "        oldkey = key\n",
    "        key = str(cutoff) + '_' + str(itr) + '_' + oldkey\n",
    "        key_mean = key + '_mean'; key_min = key + '_min'; key_max = key + '_max'; key_std = key + '_std'\n",
    "        key_sum = key + '_sum'\n",
    "\n",
    "        if len(nbrs_deg) == 0:\n",
    "            g.nodes[i][key_mean] = 0\n",
    "            g.nodes[i][key_min] = 0\n",
    "            g.nodes[i][key_max] = 0\n",
    "            g.nodes[i][key_std] = 0\n",
    "            g.nodes[i][key_sum] = 0\n",
    "        else:\n",
    "            # assert np.max(nbrs_deg) < 1.1\n",
    "            g.nodes[i][key_mean] = np.mean(nbrs_deg)\n",
    "            g.nodes[i][key_min] = np.min(nbrs_deg)\n",
    "            g.nodes[i][key_max] = np.max(nbrs_deg)\n",
    "            g.nodes[i][key_std] = np.std(nbrs_deg)\n",
    "            g.nodes[i][key_sum] = np.sum(nbrs_deg)\n",
    "\n",
    "def function_basis(g, allowed, norm_flag = 'no'):\n",
    "    # input: g\n",
    "    # output: g with ricci, deg, hop, cc, fiedler computed\n",
    "    # allowed = ['ricci', 'deg', 'hop', 'cc', 'fiedler']\n",
    "    # to save recomputation. Look at the existing feature at first and then simply compute the new one.\n",
    "\n",
    "    if len(g)<3: return\n",
    "    assert nx.is_connected(g)\n",
    "\n",
    "    def norm(g, key, flag=norm_flag):\n",
    "        if flag=='no':\n",
    "            return 1\n",
    "        elif flag == 'yes':\n",
    "            return np.max(np.abs(nx.get_node_attributes(g, key).values())) + 1e-6\n",
    "\n",
    "    if 'deg' in allowed:\n",
    "        deg_dict = dict(nx.degree(g))\n",
    "        for n in g.nodes():\n",
    "            g.nodes[n]['deg'] = deg_dict[n]\n",
    "            # g_ricci.node[n]['deg'] = np.log(deg_dict[n]+1)\n",
    "\n",
    "        deg_norm = norm(g, 'deg', norm_flag)\n",
    "        for n in g.nodes():\n",
    "            g.nodes[n]['deg'] /= np.float(deg_norm)\n",
    "    if 'deg' in allowed:\n",
    "        for n in g.nodes():\n",
    "            attribute_mean(g, n, key='deg', cutoff=1, iteration=0)\n",
    "        if norm_flag == 'yes':\n",
    "            # better normalization\n",
    "            for attr in [ '1_0_deg_sum']: # used to include 1_0_deg_std/ deleted now:\n",
    "                norm_ = norm(g, attr, norm_flag)\n",
    "                for n in g.nodes():\n",
    "                    g.nodes[n][attr] /= float(norm_)\n",
    "    return g\n",
    "\n",
    "def get_subgraphs(g, threshold=1):\n",
    "    assert str(type(g)) == \"<class 'networkx.classes.graph.Graph'>\"\n",
    "    subgraphs = [g.subgraph(c).copy() for c in sorted(nx.connected_components(g), key=len, reverse=True)]\n",
    "    subgraphs = [c for c in subgraphs if len(c) > threshold]\n",
    "    return subgraphs\n",
    "\n",
    "def new_norm(graphs_, bl_feat):\n",
    "    \"\"\"Normalize graph function uniformly\"\"\"\n",
    "    newnorm = dict(zip(bl_feat, [0] * 5))\n",
    "    for attr in bl_feat:\n",
    "        for gs in graphs_:\n",
    "            for g in gs:\n",
    "                tmp = max(nx.get_node_attributes(g, attr).values())\n",
    "                if tmp > newnorm[attr]:\n",
    "                    newnorm[attr] = tmp\n",
    "\n",
    "    for gs in graphs_:\n",
    "        for g in gs:\n",
    "            for n in g.nodes():\n",
    "                for attr in bl_feat:\n",
    "                    g.nodes[n][attr] /= float(newnorm[attr])\n",
    "                    assert g.nodes[n][attr] <=1\n",
    "    return graphs_\n",
    "\n",
    "# def save_graphs_(graphs_, dataset='imdb_binary', norm_flag='yes'):\n",
    "#     t0 = time.time()\n",
    "#     direct = os.path.join('../data/cache/', dataset, 'norm_flag_' + str(norm_flag), '')\n",
    "#     if not os.path.exists(direct): make_direct(direct)\n",
    "#     with open(direct + 'graphs_', 'wb') as f:\n",
    "#         pickle.dump(graphs_, f)\n",
    "#     print('Saved graphs. Takes %s'%(time.time() - t0))\n",
    "\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "def functionongraph(graphs_, i, key='deg', edge_flag=False):\n",
    "    # for graphs_[i], get the key-val distribution\n",
    "\n",
    "    components = len(graphs_[i]); lis = []\n",
    "    for j in range(components):\n",
    "        g = graphs_[i][j]\n",
    "        try:\n",
    "            assert (str(type(g)) ==  \"<class 'networkx.classes.graphviews.SubGraph'>\") or (str(type(g))) == \"<class 'networkx.classes.graph.Graph'>\"\n",
    "        except AssertionError:\n",
    "            if g is None:\n",
    "                print('wired case: g is None')\n",
    "                return [0]\n",
    "            else:\n",
    "                print('Unconsidered Cases in function on graph')\n",
    "\n",
    "        if edge_flag==False:\n",
    "            tmp = [g.nodes[k][key] for k in g.nodes]\n",
    "        lis += tmp\n",
    "    return lis\n",
    "\n",
    "def hisgram_single_feature(graphs_, n_bin, key='deg', his_norm_flag='yes', edge_flag=False, lowerbound=-1, upperbound=1, cdf_flag=False, uniform_flag = True):\n",
    "    import numpy as np\n",
    "    n = len(graphs_)\n",
    "    feature_vec = np.zeros((n, n_bin))\n",
    "    for i in range(n):\n",
    "        lis = functionongraph(graphs_, i, key, edge_flag=edge_flag)\n",
    "        if lis == []:\n",
    "            feature_vec[i] = 0\n",
    "        feature_vec[i] = hisgram(lis, n_bin, his_norm_flag=his_norm_flag,\n",
    "                                 lowerbound=lowerbound, upperbound=upperbound,\n",
    "                                 cdf_flag=cdf_flag, uniform_flag=uniform_flag)\n",
    "    return feature_vec\n",
    "\n",
    "def hisgram(lis, n_bin=100, his_norm_flag='yes', lowerbound=-1, upperbound=1, cdf_flag=False, uniform_flag=True):\n",
    "    if lis == []:\n",
    "        print ('lis is empty')\n",
    "        return [0]*n_bin\n",
    "    # normalize lis\n",
    "    # needs to be more rigirous\n",
    "    # TODO: test if it helps to normalize lis\n",
    "    if his_norm_flag == 'yes':\n",
    "        try:\n",
    "            assert max(lis) < 1.1 # * 100000 # delelte 100 later\n",
    "        except AssertionError:\n",
    "            print ('The max of list is %s' %max(lis)),\n",
    "        assert min(lis) > -1.1\n",
    "        max_ = max(lis)\n",
    "        # if max_ !=0:\n",
    "        #     lis = [i/float(max_) for i in lis]\n",
    "\n",
    "    if not uniform_flag:\n",
    "        assert lowerbound + 1e-3 > 0\n",
    "        n_bin_ = np.logspace(np.log(lowerbound + 1e-3), np.log(upperbound),n_bin+1, base = np.e)\n",
    "    else:\n",
    "        n_bin_ = n_bin\n",
    "\n",
    "    if cdf_flag == True:\n",
    "        from statsmodels.distributions.empirical_distribution import ECDF\n",
    "        ecdf = ECDF(lis)\n",
    "        if uniform_flag:\n",
    "            return ecdf([i / np.float(n_bin) for i in range(0, n_bin)])\n",
    "        else:\n",
    "            return ecdf([i / np.float(n_bin) for i in range(0, n_bin)])\n",
    "    result = np.histogram(lis, bins=n_bin_, range=(lowerbound,upperbound))\n",
    "    return result[0]\n",
    "\n",
    "def remove_zero_col(data, cor_flag=False):\n",
    "    # data = np.zeros((2,10))\n",
    "    # data[1,3] = data[1,5] = data[1,7] = 1\n",
    "    n_col = np.shape(data)[1]\n",
    "\n",
    "    del_col_idx = np.where(~data.any(axis=0))[0]\n",
    "    remain_col_idx = set(range(n_col)) - set(del_col_idx)\n",
    "    correspondence_dict = dict(zip(range(len(remain_col_idx)), remain_col_idx))\n",
    "    inverse_correspondence_dict = dict(zip(remain_col_idx, range(len(remain_col_idx))))\n",
    "\n",
    "    X = np.delete(data, np.where(~data.any(axis=0))[0], axis=1)\n",
    "    print('the shape after removing zero columns is ', np.shape(X))\n",
    "    if cor_flag == True:\n",
    "        return (X, correspondence_dict, inverse_correspondence_dict)\n",
    "    else:\n",
    "        return X\n",
    "\n",
    "def merge_features(graph, graphs_, allowed, n_bin=30, his_norm_flag='yes', edge_flag=False, cdf_flag=False, uniform_flag = True):\n",
    "    print('Number of bins are %s'%n_bin)\n",
    "    n = len(graphs_)\n",
    "    X = np.zeros((n, 1))\n",
    "    for key in allowed:\n",
    "        # print(key)\n",
    "        if (key=='label') :\n",
    "            if graph == 'dd_test':\n",
    "                nbin = 90\n",
    "            else:\n",
    "                nbin = 40\n",
    "            tmp = hisgram_single_feature(graphs_, nbin, 'label', his_norm_flag=his_norm_flag, edge_flag=edge_flag, lowerbound=0, upperbound=1, cdf_flag=cdf_flag, uniform_flag=uniform_flag)\n",
    "\n",
    "        elif key == 'ricciCurvature': # use default bound for ricci curvature\n",
    "            tmp = hisgram_single_feature(graphs_, n_bin, key, his_norm_flag=his_norm_flag, edge_flag=edge_flag, cdf_flag=cdf_flag, uniform_flag=uniform_flag)\n",
    "        else:\n",
    "            tmp = hisgram_single_feature(graphs_, n_bin, key, his_norm_flag=his_norm_flag, edge_flag=edge_flag, cdf_flag=cdf_flag, uniform_flag=uniform_flag, lowerbound=0)\n",
    "        X = np.append(X, tmp, axis=1)\n",
    "    return remove_zero_col(X[:,1:])\n",
    "\n",
    "import argparse\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import pickle\n",
    "import sys, os\n",
    "import time\n",
    "\n",
    "dataset = './imdb_binary'\n",
    "n_bin = 50\n",
    "nonlinear_flag = False\n",
    "uniform_flag = True\n",
    "\n",
    "bl_feat = ['1_0_deg_min', '1_0_deg_max', '1_0_deg_mean', '1_0_deg_std', 'deg']\n",
    "\n",
    "cdf_flag = True # cdf versus pdf. True for most dataset.\n",
    "norm_flag = 'no'\n",
    "his_norm_flag = 'yes'\n",
    "\n",
    "graphs, labels = load_graph(dataset)\n",
    "n = len(graphs)\n",
    "graphs_ = []\n",
    "# direct = os.path.join(dataset, 'norm_flag_' + str(norm_flag), '')\n",
    "\n",
    "# try:\n",
    "#     with open(direct + 'graphs_', 'rb') as f:\n",
    "#         t0 = time.time()\n",
    "#         graphs_ = pickle.load(f)\n",
    "#         print('Finish loading existing graphs. Takes %s'%(time.time() - t0))\n",
    "# except IOError:\n",
    "for i in range(n):\n",
    "    if i % 50 ==0: print('#'),\n",
    "    gi = convert2nx(graphs[i], i)\n",
    "    subgraphs = get_subgraphs(gi)\n",
    "    gi_s = [function_basis(gi, ['deg'], norm_flag=norm_flag) for gi in subgraphs]\n",
    "    gi_s = [g for g in gi_s if g != None]\n",
    "    graphs_.append(gi_s)\n",
    "if norm_flag == 'no': graphs_ = new_norm(graphs_, bl_feat)\n",
    "# save_graphs_(graphs_, dataset=dataset, norm_flag=norm_flag)\n",
    "\n",
    "x_original = merge_features(dataset, graphs_, bl_feat, n_bin, his_norm_flag=his_norm_flag, cdf_flag=cdf_flag, uniform_flag=uniform_flag)\n",
    "# if norm_flag=='yes':\n",
    "#     x = normalize(x_original, axis = 1)\n",
    "# else:\n",
    "#     x = x_original\n",
    "y = np.array(labels)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
