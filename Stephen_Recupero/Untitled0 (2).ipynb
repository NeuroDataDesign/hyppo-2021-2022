{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled0.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install hyppo"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RBEJuxrv5yvl",
        "outputId": "562ec387-5377-48f7-923e-8f54d5e1dd88"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting hyppo\n",
            "  Downloading hyppo-0.3.2.tar.gz (84 kB)\n",
            "\u001b[?25l\r\u001b[K     |████                            | 10 kB 16.2 MB/s eta 0:00:01\r\u001b[K     |███████▉                        | 20 kB 12.8 MB/s eta 0:00:01\r\u001b[K     |███████████▊                    | 30 kB 6.3 MB/s eta 0:00:01\r\u001b[K     |███████████████▋                | 40 kB 5.6 MB/s eta 0:00:01\r\u001b[K     |███████████████████▌            | 51 kB 4.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▍        | 61 kB 4.9 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▎    | 71 kB 4.9 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▏| 81 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 84 kB 1.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from hyppo) (1.21.6)\n",
            "Requirement already satisfied: scipy>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from hyppo) (1.4.1)\n",
            "Requirement already satisfied: numba>=0.46 in /usr/local/lib/python3.7/dist-packages (from hyppo) (0.51.2)\n",
            "Requirement already satisfied: scikit-learn>=0.19.1 in /usr/local/lib/python3.7/dist-packages (from hyppo) (1.0.2)\n",
            "Requirement already satisfied: autograd>=1.3 in /usr/local/lib/python3.7/dist-packages (from hyppo) (1.4)\n",
            "Requirement already satisfied: future>=0.15.2 in /usr/local/lib/python3.7/dist-packages (from autograd>=1.3->hyppo) (0.16.0)\n",
            "Requirement already satisfied: llvmlite<0.35,>=0.34.0.dev0 in /usr/local/lib/python3.7/dist-packages (from numba>=0.46->hyppo) (0.34.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from numba>=0.46->hyppo) (57.4.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.19.1->hyppo) (1.1.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.19.1->hyppo) (3.1.0)\n",
            "Building wheels for collected packages: hyppo\n",
            "  Building wheel for hyppo (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for hyppo: filename=hyppo-0.3.2-py3-none-any.whl size=134084 sha256=8e67970381fa2dc9cde775cbae4770e34875b5977c4ecc4b1a429d2d6d818e42\n",
            "  Stored in directory: /root/.cache/pip/wheels/a2/cf/83/86bab6230c80c120ba769dd0643db951ac795d93e5cb8ee6c5\n",
            "Successfully built hyppo\n",
            "Installing collected packages: hyppo\n",
            "Successfully installed hyppo-0.3.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "67AjMvf35s4v"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import pairwise_distances, pairwise_kernels\n",
        "from sklearn.utils import check_random_state\n",
        "from hyppo.tools import compute_dist, check_perm_blocks, check_perm_block, contains_nan, check_reps\n",
        "from joblib import Parallel, delayed\n",
        "from math import ceil\n",
        "from scipy import stats "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#all check functions taken from hyppo and adjusted for three inputs\n",
        "def check_ndarray_xyz(x, y,z):\n",
        "    \"\"\"Check if x, y, or z is an ndarray of float\"\"\"\n",
        "    if not isinstance(x, np.ndarray) or not isinstance(y, np.ndarray) or not isinstance(z, np.ndarray):\n",
        "        raise TypeError(\"x, y, and z must be ndarrays\")\n",
        "\n",
        "def convert_xyz_float64(x, y, z):\n",
        "    \"\"\"Convert x or y, or z to np.float64 (if not already done)\"\"\"\n",
        "    # convert x and y to floats\n",
        "    x = np.asarray(x).astype(np.float64)\n",
        "    y = np.asarray(y).astype(np.float64)\n",
        "    z = np.asarray(z).astype(np.float64)\n",
        "\n",
        "    return x, y, z\n",
        "\n",
        "                        \n",
        "class _CheckInputs:\n",
        "    \"\"\"Checks inputs for all independence tests\"\"\"\n",
        "\n",
        "    def __init__(self, x, y,z, reps=None):\n",
        "        self.x = x\n",
        "        self.y = y\n",
        "        self.z = z\n",
        "        self.reps = reps\n",
        "\n",
        "    def __call__(self):\n",
        "        check_ndarray_xyz(self.x, self.y, self.z)\n",
        "        contains_nan(self.x)\n",
        "        contains_nan(self.y)\n",
        "        contains_nan(self.z)\n",
        "        self.x, self.y, self.z = self.check_dim_xyz()\n",
        "        self.x, self.y, self.z = convert_xyz_float64(self.x, self.y, self.z)\n",
        "        self._check_min_samples()\n",
        "        self._check_variance()\n",
        "\n",
        "        if self.reps:\n",
        "            check_reps(self.reps)\n",
        "\n",
        "        return self.x, self.y, self.z\n",
        "    def _check_min_samples(self):\n",
        "        \"\"\"Check if the number of samples is at least 3\"\"\"\n",
        "        nx = self.x.shape[0]\n",
        "        ny = self.y.shape[0]\n",
        "        nz = self.z.shape[0]\n",
        "\n",
        "        if nx <= 3 or ny <= 3 or nz <= 3:\n",
        "            raise ValueError(\"Number of samples is too low\")\n",
        "    \n",
        "    def check_dim_xyz(self):\n",
        "        \"\"\"Convert x and y and z to proper dimensions\"\"\"\n",
        "        if self.x.ndim == 1:\n",
        "            self.x = self.x[:, np.newaxis]\n",
        "        elif self.x.ndim != 2:\n",
        "            raise ValueError(\n",
        "                \"Expected a 2-D array `x`, found shape \" \"{}\".format(self.x.shape)\n",
        "            )\n",
        "        if self.y.ndim == 1:\n",
        "            self.y = self.y[:, np.newaxis]\n",
        "        elif self.y.ndim != 2:\n",
        "            raise ValueError(\n",
        "                \"Expected a 2-D array `y`, found shape \" \"{}\".format(self.y.shape)\n",
        "            )\n",
        "        if self.z.ndim == 1:\n",
        "            self.z = self.z[:, np.newaxis]\n",
        "        elif self.z.ndim != 2:\n",
        "            raise ValueError(\n",
        "                \"Expected a 2-D array `z`, found shape \" \"{}\".format(self.z.shape)\n",
        "            )\n",
        "        return self.x, self.y, self.z\n",
        "                \n",
        "    def _check_variance(self):\n",
        "        if np.var(self.x) == 0 or np.var(self.y) == 0 or np.var(self.z) == 0:\n",
        "            raise ValueError(\"Test cannot be run, one of the inputs has 0 variance\")\n",
        "                \n",
        "\n",
        "def conditional_dcorr(x,y,z,kernel_type, reps = 1000, workers = 1,   is_distsim=True,\n",
        "        perm_blocks=None,\n",
        "        random_state=None):\n",
        "    # check matrix inputs\n",
        "    check_input = _CheckInputs(\n",
        "            x,\n",
        "            y,\n",
        "            z,\n",
        "            reps=reps\n",
        "        )\n",
        "    x, y,z = check_input()\n",
        "    #compute distance matrixes for x and y\n",
        "    distx, disty = compute_dist(\n",
        "                x, y, metric=\"euclidean\")\n",
        "    kernel_density_estimation = pairwise_kernels(z, metric=\"rbf\", n_jobs=1)\n",
        "    kernel_density_estimation= np.asarray(kernel_density_estimation).astype('float64')\n",
        "    #stat, pvalue calculations for permuting\n",
        "    statistic = Statistic(x,y,z)\n",
        "    #stat, pvalue, null_dist =  conduct_cdc_test(distx, disty, kernel_density_estimation, num_bootstrap,statistic, seed)\n",
        "    return statistic\n",
        "\n",
        "#Base Statistic function\n",
        "def Statistic(x,y,z):\n",
        "    distx, disty = compute_dist(\n",
        "                x, y, metric=\"euclidean\")\n",
        "    kernel_density_estimation = pairwise_kernels(z, metric=\"rbf\", n_jobs=1)\n",
        "    kernel_density_estimation = np.asarray(kernel_density_estimation).astype('float64')\n",
        "    return condition_distance_correlation_stats(distx, disty, kernel_density_estimation)\n",
        "\n",
        "#primary method for statsitic calculation              \n",
        "def condition_distance_correlation_stats(distance_x, distance_y, kernel_density_estimation):\n",
        "    num = distance_x.shape[0]\n",
        "    anova_x = np.zeros((num,num))\n",
        "    anova_y = np.zeros((num,num))\n",
        "    condition_distance_covariance_xy = np.zeros(num)\n",
        "    condition_distance_covariance_xx = np.zeros(num)\n",
        "    condition_distance_covariance_yy = np.zeros(num)\n",
        "\n",
        "    for i in range(num):\n",
        "        #normalization so array can be used as probabilities\n",
        "        kernel_density_estimation[i] = kernel_density_estimation[i]/kernel_density_estimation[i].sum()\n",
        "        #anovas calculated\n",
        "        anova_x = weight_distance_anova(distance_x, kernel_density_estimation[i])\n",
        "        anova_y = weight_distance_anova(distance_y, kernel_density_estimation[i])\n",
        "        #using anovas to form condition distance matrices\n",
        "        for k in range(num):\n",
        "            for j in range(num): \n",
        "                condition_distance_covariance_xy[i] += anova_x[k][j] * anova_y[k][j] * kernel_density_estimation[i][k] * kernel_density_estimation[i][j]\n",
        "                condition_distance_covariance_xx[i] += anova_x[k][j] * anova_x[k][j] * kernel_density_estimation[i][k] * kernel_density_estimation[i][j]\n",
        "                condition_distance_covariance_yy[i] += anova_y[k][j] * anova_y[k][j] * kernel_density_estimation[i][k] * kernel_density_estimation[i][j]\n",
        "    for i in range(num):\n",
        "        dcor_denominator = condition_distance_covariance_xx[i] * condition_distance_covariance_yy[i]\n",
        "        if (dcor_denominator > 0.0):\n",
        "            condition_distance_covariance_xy[i] /= np.sqrt(dcor_denominator)\n",
        "        else:\n",
        "            condition_distance_covariance_xy[i] = 0.0\n",
        "    return np.mean(condition_distance_covariance_xy)\n",
        "\n",
        "\n",
        "def weight_distance_anova(distance_matrix, weight):\n",
        "    weight_sum = np.sum(weight)\n",
        "    num = distance_matrix.shape[0]\n",
        "\n",
        "    marginal_weight_distance = np.zeros(num)\n",
        "    #construct marginal weight distance array\n",
        "    for i in range(num):\n",
        "        marginal_weight_distance[i] = vector_weight_sum(distance_matrix[i], weight)\n",
        "    weight_distance_sum = vector_weight_sum(marginal_weight_distance, weight) \n",
        "    weight_distance_sum /= weight_sum * weight_sum\n",
        "\n",
        "    for i in range(num):\n",
        "        marginal_weight_distance[i] /= weight_sum\n",
        "    #construct weight distance anova table\n",
        "    weight_distance_anova_table = np.zeros((num,num))\n",
        "    for k in range(num):\n",
        "        for j in range(num):\n",
        "            weight_distance_anova_table[k][j] = distance_matrix[k][j] - marginal_weight_distance[k] - marginal_weight_distance[j] + weight_distance_sum\n",
        "            weight_distance_anova_table[j][k] = weight_distance_anova_table[k][j]\n",
        "\n",
        "    return weight_distance_anova_table\n",
        "\n",
        "def vector_weight_sum(vector1, weight): \n",
        "    sum_value = 0.0\n",
        "    for i in range(vector1.shape[0]):\n",
        "        sum_value += vector1[i] * weight[i]\n",
        "    return sum_value\n",
        "\n",
        "#overarching function for calculating statistic and pvalue\n",
        "def conduct_cdc_test(distance_x, distance_y, kernel, statistic,seed, num_bootstrap = 99):\n",
        "    if (num_bootstrap != 0):\n",
        "        #set up default random generator\n",
        "        if (seed == None):\n",
        "            rng = np.random.default_rng()\n",
        "        else:\n",
        "            rng = np.random.default_rng(seed)\n",
        "        #acquire random saple index array for permuting\n",
        "        random_sample_index = generate_random_sample_index(num_bootstrap, kernel,rng)\n",
        "\n",
        "        bootstrap_distance_x = []\n",
        "        perm_stat = np.zeros(num_bootstrap)\n",
        "        larger_num = 0.0\n",
        "        #permute distance matrix x to acquire bootstraped distance\n",
        "        for i in range(num_bootstrap):\n",
        "            bootstrap_distance_x = rearrange_matrix(distance_x, random_sample_index[i])\n",
        "            value = Statistic(bootstrap_distance_x, distance_y, kernel)\n",
        "            perm_stat.append(value)\n",
        "            if value >= statistic:\n",
        "                larger_num = larger_num + 1\n",
        "        p_value = (1.0 + larger_num) / (1.0 + float(perm_stat.size))\n",
        "    return p_value\n",
        "\n",
        "#bootstrapped matrix calculation\n",
        "def rearrange_matrix(dist_matrix,rearrange_index):\n",
        "    new_matrix1 = np.zeros((rearrange_index.size,rearrange_index.size))\n",
        "    new_matrix2 = np.zeros((rearrange_index.size,rearrange_index.size))\n",
        "    k = 0\n",
        "    rearrange_index = np.asarray(rearrange_index).astype('int64')\n",
        "    #form matrices that have rows corresponding to rows extracted from distx at random indexes\n",
        "    for index in rearrange_index:\n",
        "        new_matrix1[k] = dist_matrix[index]\n",
        "        new_matrix2[k] = dist_matrix[index]\n",
        "        k = k + 1\n",
        "    \n",
        "    k = 0\n",
        "    #select columns from the previously computed matrices at random indexes \n",
        "    for index in rearrange_index:\n",
        "        for i in range(dist_matrix.shape[0]):\n",
        "            new_matrix1[i][k] = new_matrix2[i][index];\n",
        "        k = k + 1\n",
        "    \n",
        "    return new_matrix1\n",
        "\n",
        "def generate_random_sample_index(replication_number, probability_matrix,random_number_generator):\n",
        "    random_sample_index = np.zeros((replication_number,probability_matrix.size()))\n",
        "    #generate random indexes by sampling multinomial distribution\n",
        "    for i in range(probability_matrix.shape[0]):\n",
        "        for j in range(replication_number):\n",
        "            random_sample_index[j][i] = sample_multinomial_distribution(probability_matrix[i], random_number_generator)\n",
        "    return random_sample_index;\n",
        "\n",
        "def sample_multinomial_distribution(prob_array, random_number_generator):\n",
        "    #sampling multinomial distribution using kernel density estimation rows\n",
        "    arr = random_number_generator.multinomial(1,prob_array)\n",
        "    index = np.where(arr == 1)\n",
        "    return index[0]"
      ],
      "metadata": {
        "id": "GBxEDadK6ETB"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "a = np.array([[0,1],[2,3]])\n",
        "print(a[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BU_WnjVVSUyi",
        "outputId": "5355bb2c-f6bc-4ad9-e7c8-e8ae74d2d778"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0 1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#helper function for power calculation\n",
        "def _indep_perm_stat(x,y,z):\n",
        "    check_input = _CheckInputs(\n",
        "            x,\n",
        "            y,\n",
        "            z,\n",
        "            reps=1000\n",
        "        )\n",
        "    x,y,z = check_input()\n",
        "    obs_stat = conditional_dcorr(x,y,z,\"rbf\", reps = 1000, workers = 1,   is_distsim=True,perm_blocks=None, random_state=None)\n",
        "    distx, disty = compute_dist(\n",
        "                x, y, metric=\"euclidean\")\n",
        "    kernel_density_estimation = pairwise_kernels(z, metric=\"rbf\", n_jobs=1)\n",
        "    kernel_density_estimation= np.asarray(kernel_density_estimation).astype('float64')\n",
        "    rng = np.random.default_rng()\n",
        "    random_sample_index = np.zeros(kernel_density_estimation.shape[0])\n",
        "    #generate array of random sample indexes\n",
        "    for i in range(kernel_density_estimation.shape[0]):\n",
        "          kernel_density_estimation[i] = kernel_density_estimation[i]/kernel_density_estimation[i].sum()\n",
        "          print(kernel_density_estimation[i].sum())\n",
        "          random_sample_index[i] = sample_multinomial_distribution(kernel_density_estimation[i], rng)\n",
        "    #created bootstrap distance matrix x\n",
        "    bootstrap_distance_x = rearrange_matrix(distx, random_sample_index)\n",
        "    #calculate permutated statistic\n",
        "    perm_stat = condition_distance_correlation_stats(bootstrap_distance_x ,disty,kernel_density_estimation)\n",
        "    return obs_stat, perm_stat\n",
        "#Example 12\n",
        "def power_depend(sample_size):\n",
        "    alt_dist = []\n",
        "    null_dist = []\n",
        "    alpha = 0.05\n",
        "    for i in range(1000):\n",
        "        #dependece distribution\n",
        "        z1 = np.random.standard_t(2, sample_size)\n",
        "        x1 = z1\n",
        "        z2 = np.random.standard_t(2, sample_size)\n",
        "        x2 = z2\n",
        "        z3 = np.random.standard_t(2, sample_size)\n",
        "        x3 = z3\n",
        "        z4 = np.random.standard_t(2, sample_size)\n",
        "        x4 = z4\n",
        "        y1 = z1*z2 + (z3**2)*(z4**2)\n",
        "        y2 = z1**3 + (z2**2)*(z3*z4)\n",
        "        x = np.zeros((sample_size,4))\n",
        "        x[:,0] = x1\n",
        "        x[:,1] = x2\n",
        "        x[:,2] = x3\n",
        "        x[:,3] = x4\n",
        "        z = np.zeros((sample_size,4))\n",
        "        z[:,0] = z1\n",
        "        z[:,1] = z2\n",
        "        z[:,2] = z3\n",
        "        z[:,3] = z4\n",
        "        y = np.zeros((sample_size,2))\n",
        "        y[:,0] = y1\n",
        "        y[:,1] = y2\n",
        "        obs_stat, perm_stat = _indep_perm_stat(x,y,z)\n",
        "        print(obs_stat, 'a')\n",
        "        print(perm_stat, 'b')\n",
        "        alt_dist.append(obs_stat)\n",
        "        null_dist.append(perm_stat)\n",
        "    cutoff = np.sort(np.array(null_dist))[ceil(1000 * (1 - alpha))]\n",
        "    empirical_power = (1 + (np.array(alt_dist) >= cutoff).sum()) / (1 + 1000)\n",
        "    return empirical_power\n",
        "#Example 4\n",
        "def type_1_err_indep(sample_size):\n",
        "    alt_dist = []\n",
        "    null_dist = []\n",
        "    alpha = 0.05  \n",
        "    for i in range(1000):\n",
        "        #independece distribution\n",
        "        x1 = np.random.binomial(10,0.5,sample_size)\n",
        "        y1 = np.random.binomial(10,0.5,sample_size)\n",
        "        z1 = np.random.binomial(10,0.5,sample_size)\n",
        "        z2 = np.random.binomial(10,0.5,sample_size)\n",
        "        x = x1 + z1 + z2\n",
        "        y = y1 + z1 + z2\n",
        "        z = np.zeros((sample_size,2))\n",
        "        z[:,0] = z1\n",
        "        z[:,1] = z2\n",
        "        obs_stat, perm_stat = _indep_perm_stat(x,y,z)\n",
        "        print(obs_stat, perm_stat)\n",
        "        alt_dist.append(obs_stat)\n",
        "        null_dist.append(perm_stat)\n",
        "    cutoff = np.sort(np.array(null_dist))[ceil(1000 * (1 - alpha))]\n",
        "    type_1_err = (1 + (np.array(alt_dist) >= cutoff).sum()) / (1 + 1000)\n",
        "    return type_1_err"
      ],
      "metadata": {
        "id": "424PV7lA6QAP"
      },
      "execution_count": 20,
      "outputs": []
    }
  ]
}