{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator\n",
    "from sklearn.feature_selection import SelectorMixin\n",
    "import numpy as np\n",
    "from scipy.stats import multiscale_graphcorr\n",
    "from scipy.sparse import isspmatrix\n",
    "import warnings\n",
    "from sklearn.utils.validation import check_is_fitted\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "######################################################################\n",
    "# Scoring function\n",
    "\n",
    "# The following is a rewriting of hyppo.ksample.KSample\n",
    "# from hyppo.neurodata.io\n",
    "def k_sample_test(X, y,score_func=\"mgc\"):\n",
    "    \"\"\"Nonparametric `K`-Sample Testing test statistic.\n",
    "     \n",
    "    A k-sample test tests equality in distribution among groups. Groups\n",
    "    can be of different sizes, but must have the same dimensionality.\n",
    "    This implementation reduces the k-sample testing to an \n",
    "    independence testing problem, and leverages notable and powerful\n",
    "    multivariate independence tests.\n",
    "    \n",
    "    Read more in the :ref:`User Guide <multivariate_feature_selection>`.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X : array-like of shape (n_samples, n_features)\n",
    "        Sample vectors.\n",
    "    y : ndarray of shape (n_samples,)\n",
    "        The target vector.\n",
    "    score_func : string that refers to a multivariate independence test from scipy\n",
    "        The default and only existing test is multiscale graph correlation.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    stat : float that refers to the computed k-sample test statistic\n",
    "    \n",
    "    Notes\n",
    "    -----\n",
    "    1. The k-sample testing problem can be thought of as a generalization of\n",
    "    the two sample testing problem. \n",
    "    \n",
    "    2. By manipulating the inputs of the k-sample test, we create\n",
    "    concatenated versions of the inputs and a label matrix which are\n",
    "    paired. Then, any multivariate nonparametric test can be performed on\n",
    "    this data.\n",
    "    \n",
    "    3. Multivariate feature selection uses k-sample test score function to\n",
    "    calculate a test statistic for each feature not already selected as a \n",
    "    best feature. For each feature in that sub-section, inputted is a data matrix \n",
    "    with best features selected and that additional feature.\n",
    "    \n",
    "    References\n",
    "    ----------\n",
    "    .. [1] Sambit Panda, Cencheng Shen, Ronan Perry, Jelle Zorn, Antoine Lutz, \n",
    "           Carey E. Priebe, and Joshua T. Vogelstein. Nonpar MANOVA via \n",
    "           Independence Testing. arXiv:1910.08883 [cs, stat], April 2021. \n",
    "\n",
    "    \"\"\"\n",
    "    # extract data matrix of shape (_samples,_features) for each group\n",
    "    k_array = np.unique(y)\n",
    "    matrices = []\n",
    "    for i in k_array:\n",
    "        indices = np.where(y == i)[0] \n",
    "        if len(X.shape) == 1:\n",
    "            xi = X[indices]\n",
    "        else:\n",
    "            xi = X[indices,:]\n",
    "        matrices.append(xi)\n",
    "    X = np.concatenate(matrices)\n",
    "    # one hot encode y for multivariate independence test\n",
    "    vs = []\n",
    "    for i in range(len(np.unique(y))):\n",
    "        n = matrices[i].shape[0]\n",
    "        encode = np.zeros(shape=(n, len(matrices)))\n",
    "        encode[:, i] = np.ones(shape=n)\n",
    "        vs.append(encode)\n",
    "    y = np.concatenate(vs)\n",
    "    \n",
    "    # default, which is mgc case\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.filterwarnings(\"ignore\")\n",
    "        mgc = multiscale_graphcorr(X,y,reps = 0)\n",
    "    stat = mgc.stat \n",
    "    return stat\n",
    "\n",
    "######################################################################\n",
    "# Transformer\n",
    "\n",
    "class MultivariateFeatureSelector(SelectorMixin, BaseEstimator):\n",
    "    \"\"\" Transformer that performs forward selection.\n",
    "    \n",
    "    This feature selector adds features (forward selection) to\n",
    "    form a feature subset. At each iteration, a parallel \n",
    "    operation occurs in which a multivariate independence test \n",
    "    is performed for each data matrix with the selected best \n",
    "    features and an additional feature not yet selected. The \n",
    "    additional feature associated with the highest multivariate\n",
    "    independence test statistic is then chosen as the next best \n",
    "    feature. \n",
    "    \n",
    "    Read more in the :ref:`User Guide <multivariate_feature_selection>`.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    k: int, default=10\n",
    "        amount of features to select. \n",
    "        \n",
    "    Attributes\n",
    "    ----------\n",
    "    features_ : array, shape (n_features,)\n",
    "        indices of all features in X\n",
    "    \n",
    "    best_features_ : array, shape (k,)\n",
    "         indices of selected k best features of features_\n",
    "         \n",
    "    Examples\n",
    "    --------\n",
    "    >>> from sklearn.datasets import load_digits\n",
    "    >>> from sklearn.feature_selection import MultivariateFeatureSelector\n",
    "    >>> X, y = load_digits(return_X_y=True)\n",
    "    >>> X.shape\n",
    "    (1797, 64)\n",
    "    >>> X_new = MultivariateFeatureSelector(k = 7).fit_transform(X, y)\n",
    "    >>> X_new.shape\n",
    "    (1797, 7)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, k=10):\n",
    "        self.k = k\n",
    "        \n",
    "    def _test_stat(self, X_new, y, best_features, index):\n",
    "        # helper function for calculating, in parallel, \n",
    "        # test statistic associated with\n",
    "        # selected best features and an additional feature \n",
    "        if np.var(X_new[:,index]) == 0:\n",
    "            stat = -1.0\n",
    "        else:   \n",
    "            columns = best_features.copy() \n",
    "            columns.append(index)\n",
    "            X_j = X_new[:,columns]\n",
    "            stat = k_sample_test(X_j,y)\n",
    "        return stat\n",
    "        \n",
    "        \n",
    "    def fit(self, X, y,workers = -1):\n",
    "        \"\"\"Learn the features to select from X.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like of shape (n_samples, n_features)\n",
    "            Training vectors, where `n_samples` is the number of samples and\n",
    "            `n_features` is the number of predictors.\n",
    "        y : array-like of shape (n_samples,), default=None\n",
    "            Target values. \n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        self : object\n",
    "            Returns the instance itself.\n",
    "        \"\"\"\n",
    "        if isspmatrix(X) == True:\n",
    "            X = X.toarray()\n",
    "        \n",
    "        # array of indices that correspond to features,\n",
    "        # at each iteration, the selected best feature\n",
    "        # is removed from this array\n",
    "        features = np.arange(X.shape[1])\n",
    "        \n",
    "        if np.isnan(X).any() == True:\n",
    "            raise ValueError(\"existing multivariate independence tests in scipy do not allow nan\")\n",
    "        if type(self.k) is not int:\n",
    "            raise TypeError(\"k is type {}, must be int\".format(type(self.k)))\n",
    "        if not 0 < self.k and self.k <= X.shape[1]:\n",
    "                raise ValueError(\"k is {}, must be nonnegative <= number of features of X\".format(self.k))\n",
    "        if not X.shape[0] >= 5:\n",
    "                raise ValueError(\"number of samples is {}, must be >= 5\".format(X.shape[0]))\n",
    "        \n",
    "        # loop to select feature subset, \n",
    "        # each iteration adds next best feature as \n",
    "        # determined by the mulitivariate independence test\n",
    "        # as we rank each additional feature by statistic\n",
    "        best_features = []\n",
    "        while len(best_features) < self.k: \n",
    "            X_new = np.array(X)\n",
    "            \n",
    "            # Parallel process for test statistic calculations \n",
    "            # of selected best features and each additional feature.\n",
    "            # size of operations in parallel per loop iteration is \n",
    "            # n_features - len(best_features)\n",
    "            scores = list(\n",
    "                Parallel(n_jobs=workers)(\n",
    "                    [\n",
    "                        delayed(self._test_stat)(X_new,y,best_features,index)\n",
    "                        for index in features\n",
    "                    ]\n",
    "                )\n",
    "            )\n",
    "\n",
    "            scores_index = np.column_stack((features,np.array(scores)))\n",
    "            sorted_index = scores_index[scores_index[:,1].argsort()] \n",
    "            best = int(sorted_index[len(scores)-1,0])\n",
    "            best_features.append(best) \n",
    "            features = np.delete(features,np.where(features == best)) \n",
    "        self.best_features_ = best_features\n",
    "        self.features_ = np.arange(X.shape[1])\n",
    "        return self\n",
    "    \n",
    "    def _get_support_mask(self):\n",
    "        check_is_fitted(self)\n",
    "        return  np.array([x in self.best_features_ for x in self.features_])\n",
    "    \n",
    "    def _more_tags(self):\n",
    "        return {\"allow_nan\": False,\"requires_y\": True}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification accuracy without selecting features: 0.220\n",
      "Classification accuracy after multivariate feature selection: 0.780\n",
      "Classification accuracy after univariate feature selection: 0.480\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "==============================\n",
    "Multivariate Feature Selection\n",
    "==============================\n",
    "An example showing multivariate feature selection.\n",
    "Noisy (non informative) features are added to the wine data and\n",
    "multivariate feature selection is applied. For each feature, we plot the\n",
    "the corresponding weights of an SVM prior to and after applying \n",
    "multivariate feature selection. In the total set of features, only the \n",
    "13 first ones are significant. We can see that prior to feature selection, \n",
    "the SVM assigns greater weight to the informative features, but there\n",
    "is still a large amount of total weight assiged to the non-informative features. \n",
    "We can see from the after selection weights plot that multivariate feature selection\n",
    "selects many of the informative features which results in a greater concentration of  \n",
    "SVM total weight with the informative features, and will thus improve classification. \n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.datasets import load_wine\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "# #############################################################################\n",
    "# Import some data to play with\n",
    "X, y = load_wine(return_X_y=True)\n",
    "\n",
    "X, y = make_classification(\n",
    "        n_samples=200,\n",
    "        n_features=80,\n",
    "        n_informative=10,\n",
    "        n_redundant=5,\n",
    "        n_repeated=0,\n",
    "        n_classes=8,\n",
    "        n_clusters_per_class=3,\n",
    "        flip_y=0.0,\n",
    "        class_sep=2,\n",
    "        shuffle=False,\n",
    "        random_state=0,\n",
    "    )\n",
    "# Some noisy data not correlated\n",
    "\n",
    "\n",
    "# Add the noisy data to the informative features\n",
    "\n",
    "\n",
    "X_indices = np.arange(X.shape[-1])\n",
    "\n",
    "#split data to select features, and to evaluate classifier\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, random_state=0)\n",
    "\n",
    "#classify without multivariate feature selection\n",
    "clf = make_pipeline(MinMaxScaler(), LinearSVC())\n",
    "clf.fit(X_train, y_train)\n",
    "print(\n",
    "    \"Classification accuracy without selecting features: {:.3f}\".format(\n",
    "        clf.score(X_test, y_test)\n",
    "    )\n",
    ")\n",
    "\n",
    "\n",
    "#classifiy with multivariate feature selection\n",
    "clf_selected = make_pipeline(MultivariateFeatureSelector(10),MinMaxScaler(), RandomForestClassifier())\n",
    "clf_selected.fit(X_train, y_train)\n",
    "print(\n",
    "    \"Classification accuracy after multivariate feature selection: {:.3f}\".format(\n",
    "        clf_selected.score(X_test, y_test)\n",
    "    )\n",
    ")\n",
    "\n",
    "clf_selected = make_pipeline(SelectKBest(k = 10),MinMaxScaler(), RandomForestClassifier())\n",
    "clf_selected.fit(X_train, y_train)\n",
    "print(\n",
    "    \"Classification accuracy after univariate feature selection: {:.3f}\".format(\n",
    "        clf_selected.score(X_test, y_test)\n",
    "    )\n",
    ")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification accuracy without selecting features: 0.600\n",
      "Classification accuracy after multivariate feature selection: 0.856\n",
      "Classification accuracy after univariate feature selection: 0.760\n"
     ]
    }
   ],
   "source": [
    "X, y = load_wine(return_X_y=True)\n",
    "\n",
    "X, y = make_classification(\n",
    "        n_samples=500,\n",
    "        n_features=80,\n",
    "        n_informative=10,\n",
    "        n_redundant=5,\n",
    "        n_repeated=0,\n",
    "        n_classes=8,\n",
    "        n_clusters_per_class=2,\n",
    "        flip_y=0.0,\n",
    "        class_sep=2,\n",
    "        shuffle=False,\n",
    "        random_state=0,\n",
    "    )\n",
    "# Some noisy data not correlated\n",
    "\n",
    "\n",
    "# Add the noisy data to the informative features\n",
    "\n",
    "\n",
    "X_indices = np.arange(X.shape[-1])\n",
    "\n",
    "#split data to select features, and to evaluate classifier\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, random_state=0)\n",
    "\n",
    "#classify without multivariate feature selection\n",
    "clf = make_pipeline(MinMaxScaler(), LinearSVC())\n",
    "clf.fit(X_train, y_train)\n",
    "print(\n",
    "    \"Classification accuracy without selecting features: {:.3f}\".format(\n",
    "        clf.score(X_test, y_test)\n",
    "    )\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "#classifiy with multivariate feature selection\n",
    "clf_selected = make_pipeline(MultivariateFeatureSelector(10),MinMaxScaler(), RandomForestClassifier())\n",
    "clf_selected.fit(X_train, y_train)\n",
    "print(\n",
    "    \"Classification accuracy after multivariate feature selection: {:.3f}\".format(\n",
    "        clf_selected.score(X_test, y_test)\n",
    "    )\n",
    ")\n",
    "\n",
    "clf_selected = make_pipeline(SelectKBest(k = 10),MinMaxScaler(), RandomForestClassifier())\n",
    "clf_selected.fit(X_train, y_train)\n",
    "print(\n",
    "    \"Classification accuracy after univariate feature selection: {:.3f}\".format(\n",
    "        clf_selected.score(X_test, y_test)\n",
    "    )\n",
    ")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/svm/_base.py:986: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification accuracy without selecting features: 0.568\n",
      "Classification accuracy after multivariate feature selection: 0.992\n",
      "Classification accuracy after univariate feature selection: 0.928\n"
     ]
    }
   ],
   "source": [
    "X, y = load_wine(return_X_y=True)\n",
    "\n",
    "X, y = make_classification(\n",
    "        n_samples=500,\n",
    "        n_features=200,\n",
    "        n_informative=10,\n",
    "        n_redundant=5,\n",
    "        n_repeated=0,\n",
    "        n_classes=8,\n",
    "        n_clusters_per_class=3,\n",
    "        flip_y=0.0,\n",
    "        class_sep=4,\n",
    "        shuffle=False,\n",
    "        random_state=0,\n",
    "    )\n",
    "# Some noisy data not correlated\n",
    "\n",
    "\n",
    "# Add the noisy data to the informative features\n",
    "\n",
    "\n",
    "X_indices = np.arange(X.shape[-1])\n",
    "\n",
    "#split data to select features, and to evaluate classifier\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, random_state=0)\n",
    "\n",
    "#classify without multivariate feature selection\n",
    "clf = make_pipeline(MinMaxScaler(), LinearSVC())\n",
    "clf.fit(X_train, y_train)\n",
    "print(\n",
    "    \"Classification accuracy without selecting features: {:.3f}\".format(\n",
    "        clf.score(X_test, y_test)\n",
    "    )\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "#classifiy with multivariate feature selection\n",
    "clf_selected = make_pipeline(MultivariateFeatureSelector(10),MinMaxScaler(), RandomForestClassifier())\n",
    "clf_selected.fit(X_train, y_train)\n",
    "print(\n",
    "    \"Classification accuracy after multivariate feature selection: {:.3f}\".format(\n",
    "        clf_selected.score(X_test, y_test)\n",
    "    )\n",
    ")\n",
    "\n",
    "clf_selected = make_pipeline(SelectKBest(k = 10),MinMaxScaler(), RandomForestClassifier())\n",
    "clf_selected.fit(X_train, y_train)\n",
    "print(\n",
    "    \"Classification accuracy after univariate feature selection: {:.3f}\".format(\n",
    "        clf_selected.score(X_test, y_test)\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
