{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled0.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install hyppo"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RBEJuxrv5yvl",
        "outputId": "624a1f3b-6a71-49f3-88df-b2aad896d8cd"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting hyppo\n",
            "  Downloading hyppo-0.3.2.tar.gz (84 kB)\n",
            "\u001b[K     |████████████████████████████████| 84 kB 2.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from hyppo) (1.21.6)\n",
            "Requirement already satisfied: scipy>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from hyppo) (1.4.1)\n",
            "Requirement already satisfied: numba>=0.46 in /usr/local/lib/python3.7/dist-packages (from hyppo) (0.51.2)\n",
            "Requirement already satisfied: scikit-learn>=0.19.1 in /usr/local/lib/python3.7/dist-packages (from hyppo) (1.0.2)\n",
            "Requirement already satisfied: autograd>=1.3 in /usr/local/lib/python3.7/dist-packages (from hyppo) (1.4)\n",
            "Requirement already satisfied: future>=0.15.2 in /usr/local/lib/python3.7/dist-packages (from autograd>=1.3->hyppo) (0.16.0)\n",
            "Requirement already satisfied: llvmlite<0.35,>=0.34.0.dev0 in /usr/local/lib/python3.7/dist-packages (from numba>=0.46->hyppo) (0.34.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from numba>=0.46->hyppo) (57.4.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.19.1->hyppo) (1.1.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.19.1->hyppo) (3.1.0)\n",
            "Building wheels for collected packages: hyppo\n",
            "  Building wheel for hyppo (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for hyppo: filename=hyppo-0.3.2-py3-none-any.whl size=134084 sha256=db3a8e27af47a6b7a68f7229da8bc7aa64d26372a41611bb04a5abdfa56d9886\n",
            "  Stored in directory: /root/.cache/pip/wheels/a2/cf/83/86bab6230c80c120ba769dd0643db951ac795d93e5cb8ee6c5\n",
            "Successfully built hyppo\n",
            "Installing collected packages: hyppo\n",
            "Successfully installed hyppo-0.3.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "67AjMvf35s4v"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import pairwise_distances, pairwise_kernels\n",
        "from sklearn.utils import check_random_state\n",
        "from hyppo.tools import compute_dist, check_perm_blocks, check_perm_block, contains_nan, check_reps\n",
        "from joblib import Parallel, delayed\n",
        "from math import ceil"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def check_ndarray_xyz(x, y,z):\n",
        "    \"\"\"Check if x, y, or z is an ndarray of float\"\"\"\n",
        "    if not isinstance(x, np.ndarray) or not isinstance(y, np.ndarray) or not isinstance(z, np.ndarray):\n",
        "        raise TypeError(\"x, y, and z must be ndarrays\")\n",
        "\n",
        "def convert_xyz_float64(x, y, z):\n",
        "    \"\"\"Convert x or y, or z to np.float64 (if not already done)\"\"\"\n",
        "    # convert x and y to floats\n",
        "    x = np.asarray(x).astype(np.float64)\n",
        "    y = np.asarray(y).astype(np.float64)\n",
        "    z = np.asarray(z).astype(np.float64)\n",
        "\n",
        "    return x, y, z\n",
        "\n",
        "                        \n",
        "class _CheckInputs:\n",
        "    \"\"\"Checks inputs for all independence tests\"\"\"\n",
        "\n",
        "    def __init__(self, x, y,z, reps=None):\n",
        "        self.x = x\n",
        "        self.y = y\n",
        "        self.z = z\n",
        "        self.reps = reps\n",
        "\n",
        "    def __call__(self):\n",
        "        check_ndarray_xyz(self.x, self.y, self.z)\n",
        "        contains_nan(self.x)\n",
        "        contains_nan(self.y)\n",
        "        contains_nan(self.z)\n",
        "        self.x, self.y, self.z = self.check_dim_xyz()\n",
        "        self.x, self.y, self.z = convert_xyz_float64(self.x, self.y, self.z)\n",
        "        self._check_min_samples()\n",
        "        self._check_variance()\n",
        "\n",
        "        if self.reps:\n",
        "            check_reps(self.reps)\n",
        "\n",
        "        return self.x, self.y, self.z\n",
        "    def _check_min_samples(self):\n",
        "        \"\"\"Check if the number of samples is at least 3\"\"\"\n",
        "        nx = self.x.shape[0]\n",
        "        ny = self.y.shape[0]\n",
        "        nz = self.z.shape[0]\n",
        "\n",
        "        if nx <= 3 or ny <= 3 or nz <= 3:\n",
        "            raise ValueError(\"Number of samples is too low\")\n",
        "    \n",
        "    def check_dim_xyz(self):\n",
        "        \"\"\"Convert x and y and z to proper dimensions\"\"\"\n",
        "        if self.x.ndim == 1:\n",
        "            self.x = self.x[:, np.newaxis]\n",
        "        elif self.x.ndim != 2:\n",
        "            raise ValueError(\n",
        "                \"Expected a 2-D array `x`, found shape \" \"{}\".format(self.x.shape)\n",
        "            )\n",
        "        if self.y.ndim == 1:\n",
        "            self.y = self.y[:, np.newaxis]\n",
        "        elif self.y.ndim != 2:\n",
        "            raise ValueError(\n",
        "                \"Expected a 2-D array `y`, found shape \" \"{}\".format(self.y.shape)\n",
        "            )\n",
        "        if self.z.ndim == 1:\n",
        "            self.z = self.z[:, np.newaxis]\n",
        "        elif self.z.ndim != 2:\n",
        "            raise ValueError(\n",
        "                \"Expected a 2-D array `z`, found shape \" \"{}\".format(self.z.shape)\n",
        "            )\n",
        "        return self.x, self.y, self.z\n",
        "                \n",
        "    def _check_variance(self):\n",
        "        if np.var(self.x) == 0 or np.var(self.y) == 0 or np.var(self.z) == 0:\n",
        "            raise ValueError(\"Test cannot be run, one of the inputs has 0 variance\")\n",
        "                \n",
        "\n",
        "def conditional_dcorr(x,y,z,kernel_type, reps = 1000, workers = 1,   is_distsim=True,\n",
        "        perm_blocks=None,\n",
        "        random_state=None):\n",
        "    check_input = _CheckInputs(\n",
        "            x,\n",
        "            y,\n",
        "            z,\n",
        "            reps=reps\n",
        "        )\n",
        "    x, y,z = check_input()\n",
        "    #unsure of how to check z as check_input takes two inputs\n",
        "    distx, disty = compute_dist(\n",
        "                x, y, metric=\"euclidean\")\n",
        "    kernel_density_estimation = pairwise_kernels(z, metric=kernel_type, n_jobs=1)\n",
        "    stat = Statistic(distx,disty,kernel_density_estimation)\n",
        "    #stat, pvalue, null_dist = perm_test(Statistic, distx, disty,kernel_density_estimation,reps,workers,is_distsim,perm_blocks,random_state)\n",
        "    return stat\n",
        "\n",
        "def Statistic(distx,disty,kernel_density_estimation):\n",
        "\n",
        "    return condition_distance_correlation_stats(distx, disty, kernel_density_estimation)\n",
        "                \n",
        "def condition_distance_correlation_stats(distance_x, distance_y, kernel_density_estimation):\n",
        "    condition_distance_correlation = compute_condition_distance_correlation(distance_x, \n",
        "                                                                            distance_y,kernel_density_estimation)\n",
        "    return np.mean(condition_distance_correlation)\n",
        "\n",
        "def compute_condition_distance_correlation(distance_x, distance_y,kernel_density_estimation):\n",
        "\n",
        "    num = distance_x.shape[0]\n",
        "    anova_x = np.zeros((num,num))\n",
        "    anova_y = np.zeros((num,num))\n",
        "    condition_distance_covariance_xy = np.zeros(num)\n",
        "    condition_distance_covariance_xx = np.zeros(num)\n",
        "    condition_distance_covariance_yy = np.zeros(num)\n",
        "\n",
        "    for i in range(num):\n",
        "        anova_x = weight_distance_anova(distance_x, kernel_density_estimation[i])\n",
        "        anova_y = weight_distance_anova(distance_y, kernel_density_estimation[i])\n",
        "\n",
        "        for k in range(num):\n",
        "            for j in range(num): \n",
        "                condition_distance_covariance_xy[i] += anova_x[k][j] * anova_y[k][j] * kernel_density_estimation[i][k] * kernel_density_estimation[i][j]\n",
        "                condition_distance_covariance_xx[i] += anova_x[k][j] * anova_x[k][j] * kernel_density_estimation[i][k] * kernel_density_estimation[i][j]\n",
        "                condition_distance_covariance_yy[i] += anova_y[k][j] * anova_y[k][j] * kernel_density_estimation[i][k] * kernel_density_estimation[i][j]\n",
        "    for i in range(num):\n",
        "        dcor_denominator = condition_distance_covariance_xx[i] * condition_distance_covariance_yy[i]\n",
        "        if (dcor_denominator > 0.0):\n",
        "            condition_distance_covariance_xy[i] /= np.sqrt(dcor_denominator)\n",
        "        else:\n",
        "            condition_distance_covariance_xy[i] = 0.0\n",
        "\n",
        "    return condition_distance_covariance_xy \n",
        "\n",
        "def weight_distance_anova(distance_matrix, weight):\n",
        "    weight_sum = np.sum(weight)\n",
        "    num = distance_matrix.shape[0]\n",
        "\n",
        "    marginal_weight_distance = np.zeros(num)\n",
        "    for i in range(num):\n",
        "        marginal_weight_distance[i] = vector_weight_sum(distance_matrix[i], weight)\n",
        "    weight_distance_sum = vector_weight_sum(marginal_weight_distance, weight) \n",
        "    weight_distance_sum /= weight_sum * weight_sum\n",
        "\n",
        "    for i in range(num):\n",
        "        marginal_weight_distance[i] /= weight_sum\n",
        "\n",
        "    weight_distance_anova_table = np.zeros((num,num))\n",
        "    for k in range(num):\n",
        "        for j in range(num):\n",
        "            weight_distance_anova_table[k][j] = distance_matrix[k][j] - marginal_weight_distance[k] - marginal_weight_distance[j] + weight_distance_sum\n",
        "            weight_distance_anova_table[j][k] = weight_distance_anova_table[k][j]\n",
        "\n",
        "    return weight_distance_anova_table\n",
        "\n",
        "def vector_weight_sum(vector1, weight): \n",
        "    sum_value = 0.0\n",
        "    for i in range(vector1.shape[0]):\n",
        "        sum_value += vector1[i] * weight[i]\n",
        "    return sum_value\n",
        "\n",
        "\n",
        "class _PermNode(object):\n",
        "    \"\"\"Helper class for nodes in _PermTree.\"\"\"\n",
        "\n",
        "    def __init__(self, parent, label=None, index=None):\n",
        "        self.children = []\n",
        "        self.parent = parent\n",
        "        self.label = label\n",
        "        self.index = index\n",
        "\n",
        "    def get_leaf_indices(self):\n",
        "        if len(self.children) == 0:\n",
        "            return [self.index]\n",
        "        else:\n",
        "            indices = []\n",
        "            for child in self.children:\n",
        "                indices += child.get_leaf_indices()\n",
        "            return indices\n",
        "\n",
        "    def add_child(self, child):\n",
        "        self.children.append(child)\n",
        "\n",
        "    def get_children(self):\n",
        "        return self.children\n",
        "\n",
        "\n",
        "\n",
        "class _PermTree(object):\n",
        "    \"\"\"Tree representation of dependencies for restricted permutations\"\"\"\n",
        "\n",
        "    def __init__(self, perm_blocks):\n",
        "        perm_blocks = check_perm_blocks(perm_blocks)\n",
        "        self.root = _PermNode(None)\n",
        "        self._add_levels(self.root, perm_blocks, np.arange(perm_blocks.shape[0]))\n",
        "        indices = self.root.get_leaf_indices()\n",
        "        self._index_order = np.argsort(indices)\n",
        "\n",
        "    def _add_levels(self, root: _PermNode, perm_blocks, indices):\n",
        "        # Add new child node for each unique label, then recurse or end\n",
        "        if perm_blocks.shape[1] == 0:\n",
        "            for idx in indices:\n",
        "                child_node = _PermNode(parent=root, label=1, index=idx)\n",
        "                root.add_child(child_node)\n",
        "        else:\n",
        "            perm_block = check_perm_block(perm_blocks[:, 0])\n",
        "            for label in np.unique(perm_block):\n",
        "                idxs = np.where(perm_block == label)[0]\n",
        "                child_node = _PermNode(parent=root, label=label)\n",
        "                root.add_child(child_node)\n",
        "                self._add_levels(child_node, perm_blocks[idxs, 1:], indices[idxs])\n",
        "\n",
        "    def _permute_level(self, node, rng=None):\n",
        "        if rng is None:\n",
        "            rng = np.random\n",
        "        if len(node.get_children()) == 0:\n",
        "            return [node.index]\n",
        "        else:\n",
        "            indices, labels = zip(\n",
        "                *[\n",
        "                    (self._permute_level(child), child.label)\n",
        "                    for child in node.get_children()\n",
        "                ]\n",
        "            )\n",
        "            shuffle_children = [i for i, label in enumerate(labels) if label >= 0]\n",
        "            indices = np.asarray(indices)\n",
        "            if len(shuffle_children) > 1:\n",
        "                indices[shuffle_children] = indices[rng.permutation(shuffle_children)]\n",
        "            return np.concatenate(indices)\n",
        "\n",
        "    def permute_indices(self, rng=None):\n",
        "        return self._permute_level(self.root, rng)[self._index_order]\n",
        "\n",
        "    def original_indices(self):\n",
        "        return np.arange(len(self._index_order))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class _PermGroups(object):\n",
        "    \"\"\"Helper function to calculate parallel p-value.\"\"\"\n",
        "\n",
        "    def __init__(self, x, perm_blocks=None):\n",
        "        self.n = x.shape[0]\n",
        "        if perm_blocks is None:\n",
        "            self.perm_tree = None\n",
        "        else:\n",
        "            self.perm_tree = _PermTree(perm_blocks)\n",
        "\n",
        "    def __call__(self, rng=None):\n",
        "        if rng is None:\n",
        "            rng = np.random\n",
        "        if self.perm_tree is None:\n",
        "            order = rng.permutation(self.n)\n",
        "        else:\n",
        "            order = self.perm_tree.permute_indices(rng)\n",
        "\n",
        "        return order\n",
        "\n",
        "\n",
        "def _perm_stat(calc_stat, x, y, kernel_density_estimation, is_distsim=True, permuter=None, random_state=None):\n",
        "    \"\"\"Permute the test statistic\"\"\"\n",
        "    rng = check_random_state(random_state)\n",
        "    if permuter is None:\n",
        "        order = rng.permutation(x.shape[0])\n",
        "    else:\n",
        "        order = permuter(rng)\n",
        "\n",
        "    if is_distsim:\n",
        "        permx = x[order][:, order]\n",
        "    else:\n",
        "        permx = x[order]\n",
        "\n",
        "    perm_stat = calc_stat(permx, y, kernel_density_estimation)\n",
        "\n",
        "    return perm_stat\n",
        "\n",
        "def perm_test(\n",
        "    calc_stat,\n",
        "    x,\n",
        "    y,\n",
        "    kernel_density_estimation,\n",
        "    reps=1000,\n",
        "    workers=1,\n",
        "    is_distsim=True,\n",
        "    perm_blocks=None,\n",
        "    random_state=None):\n",
        "    \n",
        "    # calculate observed test statistic\n",
        "    stat = calc_stat(x, y, kernel_density_estimation)\n",
        "\n",
        "    # make RandomState seeded array\n",
        "    if random_state is not None:\n",
        "        rng = check_random_state(random_state)\n",
        "        random_state = rng.randint(np.iinfo(np.int32).max, size=reps)\n",
        "\n",
        "    # make random array\n",
        "    else:\n",
        "        random_state = np.random.randint(np.iinfo(np.int32).max, size=reps)\n",
        "\n",
        "    # calculate null distribution\n",
        "    permuter = _PermGroups(x, perm_blocks)\n",
        "\n",
        "    null_dist = np.array(\n",
        "        Parallel(n_jobs=workers)(\n",
        "            [\n",
        "                delayed(_perm_stat)(calc_stat, x, y, kernel_density_estimation, is_distsim, permuter, rng)\n",
        "                for rng in random_state\n",
        "            ]\n",
        "        )\n",
        "    )\n",
        "    pvalue = (1 + (null_dist >= stat).sum()) / (1 + reps)\n",
        "\n",
        "    return stat, pvalue, null_dist"
      ],
      "metadata": {
        "id": "GBxEDadK6ETB"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def _indep_perm_stat(x,y,z):\n",
        "    obs_stat = conditional_dcorr(x,y,z,\"rbf\", reps = 1000, workers = 1,   is_distsim=True,perm_blocks=None, random_state=None)\n",
        "    permx = np.random.permutation(x)\n",
        "    perm_stat = conditional_dcorr(permx,y,z,\"rbf\", reps = 1000, workers = 1,   is_distsim=True,perm_blocks=None, random_state=None)\n",
        "    return obs_stat, perm_stat\n",
        "#Example 12\n",
        "def power_depend(sample_size):\n",
        "    alt_dist = []\n",
        "    null_dist = []\n",
        "    alpha = 0.05\n",
        "    for i in range(1000):\n",
        "        z1 = np.random.standard_t(2, sample_size)\n",
        "        x1 = z1\n",
        "        z2 = np.random.standard_t(2, sample_size)\n",
        "        x2 = z2\n",
        "        z3 = np.random.standard_t(2, sample_size)\n",
        "        x3 = z3\n",
        "        z4 = np.random.standard_t(2, sample_size)\n",
        "        x4 = z4\n",
        "        y1 = z1*z2 + (z3**2)*(z4**2)\n",
        "        y2 = z1**3 + (z2**2)*(z3*z4)\n",
        "        x = np.zeros((sample_size,4))\n",
        "        x[:,0] = x1\n",
        "        x[:,1] = x2\n",
        "        x[:,2] = x3\n",
        "        x[:,3] = x4\n",
        "        z = np.zeros((sample_size,4))\n",
        "        z[:,0] = z1\n",
        "        z[:,1] = z2\n",
        "        z[:,2] = z3\n",
        "        z[:,3] = z4\n",
        "        y = np.zeros((sample_size,2))\n",
        "        y[:,0] = y1\n",
        "        y[:,1] = y2\n",
        "        obs_stat, perm_stat = _indep_perm_stat(x,y,z)\n",
        "        alt_dist.append(obs_stat)\n",
        "        null_dist.append(perm_stat)\n",
        "    cutoff = np.sort(np.array(null_dist))[ceil(1000 * (1 - alpha))]\n",
        "    empirical_power = (1 + (np.array(alt_dist) >= cutoff).sum()) / (1 + 1000)\n",
        "    return empirical_power\n",
        "#Example 3\n",
        "def type_1_err_indep(sample_size):\n",
        "    alt_dist = []\n",
        "    null_dist = []\n",
        "    alpha = 0.05  \n",
        "    for i in range(1000):\n",
        "        x1 = np.random.binomial(10,0.5,sample_size)\n",
        "        y1 = np.random.binomial(10,0.5,sample_size)\n",
        "        z1 = np.random.binomial(10,0.5,sample_size)\n",
        "        z2 = np.random.binomial(10,0.5,sample_size)\n",
        "        x = x1 + z1 + z2\n",
        "        y = y1 + z1 + z2\n",
        "        z = np.zeros((sample_size,2))\n",
        "        z[:,0] = z1\n",
        "        z[:,1] = z2\n",
        "        obs_stat, perm_stat = _indep_perm_stat(x,y,z)\n",
        "        alt_dist.append(obs_stat)\n",
        "        null_dist.append(perm_stat)\n",
        "    cutoff = np.sort(np.array(null_dist))[ceil(1000 * (1 - alpha))]\n",
        "    type_1_err = (1 + (np.array(alt_dist) >= cutoff).sum()) / (1 + 1000)\n",
        "    return type_1_err"
      ],
      "metadata": {
        "id": "424PV7lA6QAP"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(power_depend(50))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Iamok6vt8Mc0",
        "outputId": "d159583b-eb38-4d14-c3c0-45c7277b9490"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(type_1_err_indep(50))"
      ],
      "metadata": {
        "id": "tuEsgqY2dYN0"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}